数据获取
数据清洗 
数据种类
数据集概要
实体规模，实体关系规模，回答xx类问题，实体类型，属性类型，实体关系类型 （gittub上表格）
构建知识图谱  

一.基于模板匹配的问答模型

    基于规则匹配实现，通过关键词匹配，对问句进行分类，医疗问题本身属于封闭域类场景，
    对领域问题进行穷举并分类，
    然后使用cypher的match去匹配查找neo4j，根据返回数据组装问句回答，最后返回结果。

    整体大概流程： 首先传入用户输入问题，调用self.classifier.classify进行问句分类，如果没有对应的分类结果，则输出模板句式。如果有分类结果，则调用self.parser.parser_main对问句进行解析，再调用self.searcher.search_main查找对应的答案，如果有则返回答案，如果没有则输出模板句式。

    1.问句分类 分类型
    功能：获取问句中包含的领域词及其所在领域，并收集问句当中所涉及到的实体类型；
    流程：首先调用check_medical函数，获取问句中包含的领域词及其所在领域，并收集问句当中所涉及到的实体类型；
    接着基于特征词进行分类，即调用check_word函数，看问句中是否包含某领域特征词，以及该领域是否在问句中包含的region_words的实体类型（types）里，以此来判断问句属于哪种类型。
    比如：如果没有查到若没有查到相关的外部查询信息，且类型为疾病，那么则将该疾病的描述信息返回（question_types = ['disease_desc']）；若类型为症状，那么则将该症状的对应的疾病信息返回（question_types = ['symptom_disease']）。
    然后将分类结果进行合并处理，组装成一个字典返回。

    构建领域actree：加速过滤。通过python的ahocorasick库实现。ahocorasick是一种字符串匹配算法，由两种数据结构实现：trie和Aho-Corasick自动机。
                    Trie是一个字符串索引的词典，检索相关项时时间和字符串长度成正比。
                    AC自动机能够在一次运行中找到给定集合所有字符串。AC自动机其实就是在Trie树上实现KMP，可以完成多模式串的匹配。

    构建词典：根据xx类实体构造 {特征词：特征词对应类型} 词典
    匹配领域词：通过ahocorasick库的iter()函数匹配领域词，将有重复字符串的领域词去除短的，取最长的领域词返回。功能为过滤问句中含有的领域词，返回{问句中的领域词：词所对应的实体类型}。
                接着基于特征词进行分类，即调用check_word函数，看问句中是否包含某领域特征词，以及该领域是否在问句中包含的region_words的实体类型（types）里，以此来判断问句属于哪种类型。
    实体识别：检查问句中是否含有某实体类型内的特征词。


    2.问句解析

    流程：首先传入问句分类结果，获取问句中领域词及其实体类型。
            接着调用build_entitydict函数，返回形如{'实体类型':['领域词'],...}的entity_dict字典。
            然后对问句分类返回值中[‘question_types’]的每一个question_type，调用sql_transfer函数转换为neo4j的Cypher语言。
            最后组合每种question_type转换后的sql查询语句。
    解析分类结果：（举例）
    转换Cypher查询语言：不同的问题类型，转换为Cypher查询语言并返回。
        （举例）

    3.查询结果
    流程：
    查询模块：传入问题解析的结果sqls，将保存在queries里的[‘question_type’]和[‘sql’]分别取出，通过图数据库neo4j查询得到结果
    回复模块：根据不同问题类型，调用相应的回复模板
    （举例）

    4. 缺失实体填充：
        在用户连续提问时，若某轮对话中没有证券相关的实体作为主体，默认采用上一轮对话中的主体。
        
二. 基于FastText改进的问答模型
    文本分类：
    1.数据获取： 今日头条数据集
               共32000条，分布于15个类别。每行为一条数据，以_!_分割的个字段。从前往后分别是新闻ID，分类code，类别，新闻字符串（仅含标题），新闻关键词。
    2.数据预处理： 
        工具： langconv 用于中文繁体/简体转换的包，jieba 中文分词工具
        步骤：
            在数据集的TXT文档中的格式是以_!_来进行画划分的，根据题目条件，若要进行数据集的冗余信息的去除和保留新闻内容（关键词），就要先对_!_进行识别并划分
            整理停用词：将停用词txt文件转为列表
            将数据集中的繁体转为简体
            通过jieba工具数据集中的文本去除特殊符号，停用词，分词
            通过pandas库从csv文件中读取dataframe,再将数据集转为txt形式
            由于数据是规则排列的，直接划分势必会导致数据集的不平衡。需要先进行随机的排列生成一个新的数据，在对该数据进行划分。
            实现原理：对于获得的数据的索引index进行随机排列，在对索引进行重新建立，根据索引对于数据进行划分按照8:2比例将数据集分割为训练集与验证集
            统计数据集中不同种类的数量，将每一类的标签的样本扩充到xx数量
    
    3.训练
        工具： fasttext ,sklearn
        步骤：
            设定模型参数
                :param dim: 词向量大小， 默认100
                :param epoch: 默认100
                :param lr: 学习率， 默认0.5
                :param loss: 损失函数，默认softmax, 多分类问题推荐 ova
                :param wordNgrams: 默认2
            开始训练

    4.模型验证
      使用 classification_report 验证 fasttext 模型分类效果，结果如下表
      (表格)



